using Crux, Distributions, POMDPGym, Plots
include("utils.jl")

## Parameters
# Experiment params
Neps=20000

# Problem setup params
failure_target=0.7
mdp = ContinuousBanditMDP(0, 1) #NOTE: To get multimodal behavior, need to modify POMDPGym impl (add a 1- to the reward)
Px = DistributionPolicy(Normal(0, 0.5))

xi, wi = quadrature_pts(Px.distribution.Î¼, Px.distribution.Ïƒ)

S = state_space(mdp)
A = action_space(Px)

# Show returns
D = episodes!(Sampler(mdp, Px), Neps=1000)
histogram(D[:r][:])

# Networks
net(out_act...;Nin=1, Nout=1) = Chain(Dense(Nin, 12, relu), Dense(12,12, relu), Dense(12, Nout, out_act...)) # Basic architecture
V() = ContinuousNetwork(net(sigmoid)) # Value network
function Î ()
	base = net(Nout=2)
	Î¼ = ContinuousNetwork(Chain(base..., Dense(2, 1)))
	logÎ£ = ContinuousNetwork(Chain(base..., Dense(2, 1)))
	GaussianPolicy(Î¼, logÎ£, true)
end
function Î _mixture()
	base = net(Nout=2)
	
	mu1 = ContinuousNetwork(Chain(base..., Dense(2, 1)))
    logÎ£1 = ContinuousNetwork(Chain(base..., Dense(2, 1)))
	
    mu2 = ContinuousNetwork(Chain(base..., Dense(2, 1)))
    logÎ£2 = ContinuousNetwork(Chain(base..., Dense(2, 1)))
	
    Î±s = ContinuousNetwork(Chain(base..., Dense(2, 2), softmax), 2)
    MixtureNetwork([GaussianPolicy(mu1, logÎ£1, true), GaussianPolicy(mu2, logÎ£2, true)], Î±s)
end
AC(A=Î ()) = ActorCritic(A, V()) # Actor-crtic for baseline approaches
QSA() = ContinuousNetwork(net(sigmoid, Nin=2)) # Q network for value-based
AQ(A=Î ()) = ActorCritic(A, QSA()) # Actor-critic for continuous value-based approach




# Solver parameters
Nbuff = Neps
shared_params(name, Ï€) = (agent=PolicyParams(;Ï€, pa=Px), 
                          N=Neps, 
                          S,
                          f_target=failure_target,
                          buffer_size=Nbuff,
                          log=(dir="log/$name", period=1000))
						  
mc_params(N=Neps) = (agent=PolicyParams(;Ï€=Px, pa=Px), 
			 N=N, 
			 S, 
			 buffer_size=N, 
			 ğ’«=(;f_target=failure_target))
			 
pg_params(name, Ï€, use_baseline=false) = (Î”N=200, 
										  use_baseline,
										  training_buffer_size=200,
										  agent_pretrain=pretrain_policy(mdp, Px),
										  shared_params(name, Ï€)...)
vb_params(name, Ï€) = (Î”N=4,
					  train_actor=true,
					  c_opt=(epochs=4,), 
					  agent_pretrain=pretrain_AQ(mdp, Px, v_target=0.1),
					  training_buffer_size=5000,
					  xi, 
					  wi,
					  shared_params(name, Ï€)...)


## Policy gradient MIS approach 
ğ’® = PolicyGradientIS(;pg_params("PG", Î ())...)
solve(ğ’®, mdp)
plot(-4:0.02:4, (a) -> reward(mdp, [0], a))
hline!([failure_target])
a1 = action(ğ’®.agent.Ï€, ğ’®.buffer[:s])
histogram!(a1[:], normalize=true)


ğ’® = ValueBasedIS(;vb_params("VB", AQ())...)
solve(ğ’®, mdp)
plot(-4:0.02:4, (a) -> reward(mdp, [0], a))
hline!([failure_target])
plot!(-4:0.02:4, (a) -> value(ğ’®.agent.Ï€, [-1.], [a])[1])
a1 = action(ğ’®.agent.Ï€, ğ’®.buffer[:s])
histogram!(a1[:], normalize=true)


ğ’® = PolicyGradientIS(;pg_params("PG_MIS", MISPolicy([Î (), Î (), Î (), Î ()], [1/4, 1/4, 1/4, 1/4]))...)
fs, ws = solve(ğ’®, mdp)

plot(-4:0.02:4, (a) -> reward(mdp, [0], a))
hline!([failure_target])
a1 = action(ğ’®.agent.Ï€.distributions[1], ğ’®.buffer[:s])
a2 = action(ğ’®.agent.Ï€.distributions[2], ğ’®.buffer[:s])
a3 = action(ğ’®.agent.Ï€.distributions[3], ğ’®.buffer[:s])
a4 = action(ğ’®.agent.Ï€.distributions[4], ğ’®.buffer[:s])
histogram!(a1[:], normalize=true)
histogram!(a2[:], normalize=true)
histogram!(a3[:], normalize=true)
histogram!(a4[:], normalize=true)

## Policy gradient Mixture approach 
ğ’® = PolicyGradientIS(;pg_params("PG_mixture", Î _mixture())..., agent_pretrain=nothing)
fs, ws = solve(ğ’®, mdp)

plot(-4:0.02:4, (a) -> reward(mdp, [0], a))
hline!([failure_target])
a1 = action(ğ’®.agent.Ï€.networks[1], ğ’®.buffer[:s])
a2 = action(ğ’®.agent.Ï€.networks[2], ğ’®.buffer[:s])
histogram!(a1[:], normalize=true)
histogram!(a2[:], normalize=true)

## Value based MIS approach
ğ’® = ValueBasedIS(;vb_params("VB_MIS", MISPolicy([AQ(), AQ()], [0.5, 0.5]))...)
# ğ’® = ValueBasedIS(;vb_params("VB", AQ())...)
fs, ws = solve(ğ’®, mdp)


plot(-4:0.02:4, (a) -> reward(mdp, [0], a))
hline!([failure_target])
plot!(-4:0.02:4, (a) -> value(ğ’®.agent.Ï€.distributions[1], [-1.], [a])[1])
plot!(-4:0.02:4, (a) -> value(ğ’®.agent.Ï€.distributions[2], [-1.], [a])[1])
a1 = action(actor(ğ’®.agent.Ï€).distributions[1], zeros(1,1000))
a2 = action(actor(ğ’®.agent.Ï€).distributions[2], zeros(1,1000))
histogram!(a1[:], normalize=true)
histogram!(a2[:], normalize=true)


## Value-based mixture approach
ğ’® = ValueBasedIS(;vb_params("VB_Mixture", AQ(Î _mixture()))...)
fs, ws = solve(ğ’®, mdp)

plot(-4:0.02:4, (a) -> reward(mdp, [0], a))
hline!([failure_target])
plot!(-4:0.02:4, (a) -> value(ğ’®.agent.Ï€, [-1.], [a])[1])
a1 = action(ğ’®.agent.Ï€.A.networks[1], zeros(1,1000))
a2 = action(ğ’®.agent.Ï€.A.networks[2], zeros(1,1000))
histogram!(a1[:], normalize=true)
histogram!(a2[:], normalize=true)

