include("evaluation.jl")

function pgis_loss(Ï€, ð’«, ð’Ÿ; info = Dict())
	Ï€s = trainable_policies(Ï€)
	
	# Compute the baseline
	b = ignore_derivatives() do
		if ð’«[:use_baseline]
			if length(Ï€s) > 1
				bs = vcat([logpdf(d, ð’Ÿ[:s], ð’Ÿ[:a]) for d in Ï€s]...)
				b = sum(bs .* oh, dims=1)
			else
				b = value(Ï€s[1], ð’Ÿ[:s])
			end
		else
			b = 0f0
		end
		b
	end
	
	# Compute the log probability
	if length(Ï€s) > 1
		ids = ð’Ÿ[:id][:]
		oh = Flux.onehotbatch(ð’Ÿ[:id][:], collect(1:length(Ï€s)))
		logpdfs = vcat([logpdf(d, ð’Ÿ[:s], ð’Ÿ[:a]) for d in Ï€s]...)
		new_probs = sum(logpdfs .* oh, dims=1)
		target = reshape(ð’«[:f_target_train][ids], 1, :)
	else
		new_probs = logpdf(Ï€s[1], ð’Ÿ[:s], ð’Ÿ[:a])
		target = ð’«[:f_target_train][1]
	end
	
	# Log relevant parameters
	ignore_derivatives() do
		info[:kl] = mean(ð’Ÿ[:logprob] .- new_probs)
		info[:mean_baseline] = mean(b)
	end 
	
	-mean(new_probs .* ((ð’Ÿ[:return] .> target) .* ð’Ÿ[:traj_importance_weight] .- b))
end

function value_loss(Ï€, ð’«, D; kwargs...)
	Ï€s = trainable_policies(Ï€)
	if length(Ï€s) > 1
		ids = ð’Ÿ[:id][:]
		valss = vcat([value(d, D[:s]) for d in Ï€s]...)
		vals = sum(valss .* Flux.onehotbatch(ids, collect(1:length(Ï€s))), dims=1)
	else
		vals = value(Ï€s[1], D[:s])
	end
	
	returns = (D[:return] .> ð’«[:f_target_train][1]) .* D[:traj_importance_weight]
	
	Flux.mse(vals, returns)
end

function PolicyGradientIS(;agent::PolicyParams,
			  Î”N,
			  use_baseline=false,
			  elite_frac=0.1,
			  target_kl = 0.015,
			  f_target,
              a_opt::NamedTuple=(;), 
              c_opt::NamedTuple=(;), 
              log::NamedTuple=(;), 
              required_columns=[],
			  training_buffer_size,
			  buffer_size,
			  name = "pgis",
              kwargs...)
    if use_baseline
		name = string(name, "_baseline")
		c_opt=TrainingParams(;loss=value_loss, name = "critic_", c_opt...)
	else
		c_opt = nothing
	end
	f_target_train = [f_target]
	# If MIS, make sure we record an ID. 
	if agent.Ï€ isa MISPolicy
		push!(required_columns, :id)
		f_target_train = fill(f_target, length(trainable_policies(agent.Ï€)))
	end
    EvaluationSolver(;agent=agent,
                    ð’«=(;use_baseline, elite_frac, f_target=[f_target], f_target_train),
					buffer_size,
					training_type=:policy_gradient,
                    log=LoggerParams(;dir = "log/$name", log...),
                    a_opt=TrainingParams(;loss=pgis_loss, early_stopping = (infos) -> (infos[end][:kl] > target_kl), name = "actor_", max_batches=1000, a_opt...),
                    c_opt=c_opt,
                    required_columns = unique([required_columns..., :logprob, :return, :traj_importance_weight]),
					pre_train_callback=gradual_target_increase,
                    kwargs...)
end
        
    



