include("evaluation.jl")

# Sample from a discrete value function according to the optimal distribution
function estimator_logits(P)
	(Ï€, s) -> begin
	    vals = value(Ï€, s)
	    probs = Float32.(Crux.logits(P, s))
	    ps = vals .* probs
		sm = sum(ps, dims=1)
	    zeros = sm[:] .== 0
		sm[zeros] .= 1f0
		ps[:, zeros] .= probs
	    ps ./ sm
	end
end

# Estimate the value function for a discrete network
function value_estimate(Ï€::DiscreteNetwork, px, s)
	pdfs = Float32.(Crux.logits(px, s))
    sum(value(Ï€, s) .* pdfs, dims=1)
end

function Ef_target(Ï€, ğ’«, ğ’Ÿ; kwargs...)
	Ï€s = trainable_policies(Ï€)
	if length(Ï€s) > 1
		ids = ğ’Ÿ[:id][:]
		all_values = vcat([value_estimate(d, ğ’«[:px], ğ’Ÿ[:sp]) for d in Ï€s]...)
		values = sum(all_values .* Flux.onehotbatch(ids, collect(1:length(Ï€s))), dims=1)
	else
		values = value_estimate(Ï€s[1], ğ’«[:px], ğ’Ÿ[:sp])
	end
	return ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> ğ’«[:f_target_train][1]) .+ (1.f0 .- ğ’Ÿ[:done]) .* values
end

function td_loss_mis(;loss=Flux.mse, name=:Qavg, s_key=:s, a_key=:a, weight=nothing)
    (Ï€, ğ’«, ğ’Ÿ, y; info=Dict()) -> begin
		Ï€s = trainable_policies(Ï€)
		if length(Ï€s) > 1
			ids = ğ’Ÿ[:id][:]
			Qs = vcat([value(d, ğ’Ÿ[s_key], ğ’Ÿ[a_key]) for d in Ï€s]...)
			Q = sum(Qs .* Flux.onehotbatch(ids, collect(1:length(Ï€s))), dims=1)
		else
			Q = value(Ï€s[1], ğ’Ÿ[s_key], ğ’Ÿ[a_key])
		end
		
        # Store useful information
        ignore_derivatives() do
            info[name] = mean(Q)
        end
        
        loss(Q, y, agg = isnothing(weight) ? mean : Crux.weighted_mean(ğ’Ÿ[weight]))
    end
end

function ValueBasedIS(;agent::PolicyParams,
			  train_actor=false,
			  elite_frac = 0.1,
			  N_elite_candidate=100,
			  f_target,
			  Î”N,
              a_opt::NamedTuple=(;), 
              c_opt::NamedTuple=(;), 
              log::NamedTuple=(;), 
              required_columns=[],
			  recent_batch_only=false,
			  name = "value_based_is",
              kwargs...)
    if train_actor
		@error "not implemented"
	else
		a_opt = nothing
	end
	
	# If MIS, make sure we record an ID. 
	if agent.Ï€ isa MISPolicy
		push!(required_columns, :id)
	end
	
    EvaluationSolver(;agent=PolicyParams(agent, Ï€â»=deepcopy(agent.Ï€)),
                    ğ’«=(;px=agent.pa, f_target=[f_target], f_target_train=[f_target], elite_frac, N_elite_candidate),
					Î”N=Î”N,
					training_type=:value,
					recent_batch_only=recent_batch_only,
                    log=LoggerParams(;dir = "log/$name", log...),
                    a_opt=a_opt,
					c_opt=TrainingParams(;loss=td_loss_mis(weight=:fwd_importance_weight), name="critic_", epochs=Î”N, c_opt...),
                    target_fn=Ef_target,
                    required_columns=unique([required_columns..., :return, :traj_importance_weight, :fwd_importance_weight, :importance_weight]),
					pre_train_callback=gradual_target_increase,
                    kwargs...)
end
        
