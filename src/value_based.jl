include("evaluation.jl")

# Sample from a discrete value function according to the optimal distribution
function estimator_logits(P)
	(Ï€, s) -> begin
	    vals = value(Ï€, s)
	    probs = Float32.(Crux.logits(P, s))
	    ps = vals .* probs
		sm = sum(ps, dims=1)
	    zeros = sm[:] .== 0
		sm[zeros] .= 1f0
		ps[:, zeros] .= probs
	    ps ./ sm
	end
end

# Estimate the value function for a discrete network
function value_estimate(Ï€::DiscreteNetwork, s, ğ’«)
	pdfs = Float32.(Crux.logits(ğ’«[:px], s))
    sum(value(Ï€, s) .* pdfs, dims=1)
end

# Estimate the value function for a Continuous network
function value_estimate(Ï€::ActorCritic, s, ğ’«)
	# samps = []
	# for i=1:ğ’«[:N_samples]
	# 	anom, _ = exploration(ğ’«[:px], s)
	# 	# a, logqa = exploration(Ï€, s)
	# 	# w = exp.(logpdf(ğ’«[:px], s, a) .- logqa)
	# 	# push!(samps, value(Ï€, s, a) .* w)
	# 	push!(samps, value(Ï€, s, anom))
	# end
	# return mean(samps)
	samps = []
	for a in ğ’«[:xi]
		push!(samps, value(Ï€, s, repeat(a, 1, size(s)[end])))
	end
	
	return sum(ğ’«[:wi] .* samps)
end

function Ef_target(Ï€, ğ’«, ğ’Ÿ; kwargs...)
	values = value_estimate(Ï€, ğ’Ÿ[:sp], ğ’«)
	values[ğ’Ÿ[:done]] .= 0f0
	
	return ğ’Ÿ[:done] .* (ğ’Ÿ[:r] .> ğ’«[:f_target_train_current][1]) .+ (1.f0 .- ğ’Ÿ[:done]) .* values
end

function actor_loss_continuous_is(Ï€, ğ’«, ğ’Ÿ; info=Dict())
	a, logqa = exploration(Ï€, ğ’Ÿ[:s])
	
	# Compute the estimated value at each state and the fwd importance weight for visitation frequency
	f_s, ws = ignore_derivatives() do
		fs = value_estimate(Ï€, ğ’Ÿ[:s], ğ’«) .+ 1f-20
		
		ws = ğ’Ÿ[:fwd_importance_weight] ./ ğ’Ÿ[:importance_weight]
		ws[ğ’Ÿ[:importance_weight] .== 0f0] .= 0f0
		
		fs, ws
	end
	
	f_sa = value(Ï€, ğ’Ÿ[:s], a) .+ 1f-20
	qa = exp.(logqa)
	pa = exp.(logpdf(ğ’«[:px], ğ’Ÿ[:s], a)) 
	
	# Option 1: DKL(qÎ¸ || qstar)
	# qstar = f_sa .* pa ./ f_s
	# mean( ws .* (log.(qa ./ qstar)))
	
	# Option 2: DKL(qstar || qÎ¸)
	-mean(ws .* f_sa .* pa .* logqa ./ (qa .* f_s))
end

function ValueBasedIS(;agent::PolicyParams,
			  prioritized = true,
			  training_buffer_size,
			  buffer_size,
			  priority_fn=Crux.td_error,
			  train_actor=false,
			  elite_frac = 0.1,
			  xi, # quadrature pts
			  wi, # quadrature weights
			  f_target,
			  Î”N,
              a_opt::NamedTuple=(;), 
              c_opt::NamedTuple=(;), 
              log::NamedTuple=(;), 
              required_columns=Symbol[],
			  name = "value_based_is",
              kwargs...)
    if train_actor
		a_opt=TrainingParams(;loss=actor_loss_continuous_is, name="actor_", a_opt...)
	else
		a_opt = nothing
	end
	
	f_target_train = fill(0f0, length(all_policies(agent.Ï€)))
	
	# If MIS, make sure we record an ID. 
	if agent.Ï€ isa MISPolicy
		push!(required_columns, :id)
		
	end
	
	required_columns=unique([required_columns..., :return, :traj_importance_weight, :fwd_importance_weight, :importance_weight])
	
    EvaluationSolver(;agent=PolicyParams(agent, Ï€â»=deepcopy(agent.Ï€)),
                    ğ’«=(;px=agent.pa, elite_frac, f_target=[f_target], f_target_train, f_target_train_current=[0f0], wi, xi),
					Î”N,
					training_buffer_size,
					buffer_size,
					required_columns,
					buffer = ExperienceBuffer(S, agent.space, buffer_size, required_columns, prioritized=prioritized),
					training_type=:value,
                    log=LoggerParams(;dir = "log/$name", log...),
                    a_opt=a_opt,
					# c_opt=TrainingParams(;loss=Crux.td_loss(weight=:fwd_importance_weight), name="critic_", epochs=Î”N, c_opt...),
					c_opt=TrainingParams(;loss=Crux.td_loss(), name="critic_", epochs=Î”N, c_opt...),
                    target_fn=Ef_target,
					priority_fn,
                    kwargs...)
end
        
