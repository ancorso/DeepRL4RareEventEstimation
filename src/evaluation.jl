using Parameters, Random
import Zygote: ignore_derivatives

function safe_weight_fn(agent, data, ep)
	logp = trajectory_logpdf(agent.pa, data, ep)
	logq = trajectory_logpdf(agent.Ï€, data, ep)
	if logq == -Inf
		return 0f0
	else
		return exp(logp - logq)
	end	
end

@with_kw mutable struct EvaluationSolver <: Solver
    agent::PolicyParams # Policy
    S::AbstractSpace # State space
    N::Int = 1000 # Number of episode samples
    Î”N::Int = 200 # Number of episode samples between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    i::Int = 0 # The current number of episode interactions
    a_opt::Union{Nothing, TrainingParams} = nothing# Training parameters for the actor
    c_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the critic
    ğ’«::NamedTuple = (;) # Parameters of the algorithm
    post_sample_callback = (ğ’Ÿ; kwargs...) -> nothing # Callback that that happens after sampling experience
    pre_train_callback = (ğ’®; kwargs...) -> nothing # callback that gets called once prior to training
    required_columns = Symbol[:traj_importance_weight, :return]
	
	# Stuff specific to estimation
	training_type = :none # Type of training loop, either :policy_gradient, :value, :none
	training_buffer_size = Î”N*max_steps # Whether or not to train on all prior data or just the recent batch
	weight_fn = safe_weight_fn
	agent_pretrain=nothing # Function to pre-train the agent before any rollouts

	# Create buffer to store all of the samples
	buffer_size = N*max_steps
	buffer = ExperienceBuffer(S, agent.space, buffer_size, required_columns)
	ğ’Ÿ = nothing

    # Stuff for off policy update
    target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0) # Function for updating the target network
    target_fn=nothing # Target for critic regression with input signature (Ï€â», ğ’Ÿ, Î³; i)
    priority_fn = Crux.td_error  # function for prioritized replay
end

MCSolver(args...; kwargs...) = EvaluationSolver(args...; kwargs...)
function CEMSolver(; agent, 
					 Î”N, 
					 log::NamedTuple=(;), 
					 required_columns=Symbol[], 
					 elite_frac=0.1, 
					 f_target,
					 buffer_size,
					 name="cem",
					 kwargs...)
					 
	f_target_train = fill(0f0, length(all_policies(agent.Ï€)))
	# If MIS, make sure we record an ID. 
	agent.Ï€ isa MISPolicy && push!(required_columns, :id)
	required_columns = unique([required_columns..., :logprob, :return, :traj_importance_weight])
	ğ’«=(;elite_frac, f_target=[f_target], f_target_train)
	
	EvaluationSolver(;agent,
					  Î”N,
					  buffer_size, 
					  required_columns, 
					  ğ’«,
					  training_type=:cem, 
					  log=LoggerParams(;dir = "log/$name", log...),
					  kwargs...)
end

function gradual_target_increase(ğ’®, ğ’Ÿ; info)
	fs = ğ’Ÿ[:return][ğ’Ÿ[:episode_end] .== true][:]
	Ï€t = trainable_indices(ğ’®.agent.Ï€)
	
	# In the MIS case, we have to set targets for different modes separately
	if ğ’®.agent.Ï€ isa MISPolicy
		ids = ğ’Ÿ[:id][ğ’Ÿ[:episode_end] .== true][:]
		for (id, Ï€) in enumerate(all_policies(ğ’®.agent.Ï€))
			fs_id = fs[ids .== id]
			if length(fs_id) == 0 # If there are no samples associated with a policy, set its target back to 0f0
				target = 0f0
			elseif id in Ï€t # for trainable distributions, gradually increase the target		
				elite_target_min = sort(fs_id, rev=true)[max(1, floor(Int, length(fs_id) * ğ’®.ğ’«[:elite_frac]))]
				target = min(elite_target_min, ğ’®.ğ’«[:f_target][1])
			else # for non trainable distributions, just set the real target
				target = ğ’®.ğ’«[:f_target][1]
			end
			info["f_target_train_$id"] = target
			ğ’®.ğ’«[:f_target_train][id] = target
		end
	else
		elite_target_min = sort(fs, rev=true)[max(1, floor(Int, length(fs) * ğ’®.ğ’«[:elite_frac]))]
		target = min(elite_target_min, ğ’®.ğ’«[:f_target][1])
		info[:f_target_train] = target
		ğ’®.ğ’«[:f_target_train][1] = target
	end
end

function assign_mode_ids(ğ’®, ğ’Ÿ; info=Dict())
	Ï€s = all_policies(ğ’®.agent.Ï€)
	length(Ï€s) == 1 && return # If there is only one trainable policy,
	
	mis = ğ’®.agent.Ï€
	
	weights = mis.weights
	logweights = log.(weights)
	new_weights = zeros(Float32, length(weights)) # new set of weights
	
	eps = episodes(ğ’Ÿ)
	for ep_t in eps
		ep = ep_t[1]:ep_t[2]
		# update the failure mode id of each sample (this might change over time as well)
		
		# Get the likelihood that the sample "ep" comes from each policy
		pw = [trajectory_logpdf(d, ğ’Ÿ, ep) for d in Ï€s] .+ logweights # likelihood times weight
		# Î³ = sum(pw) == 0 ? weights : pw ./ sum(pw) # normalize (safely)
		id = argmax(pw)
		ğ’Ÿ[:id][1, ep] .= id
		new_weights[id] += ğ’Ÿ[:traj_importance_weight][1, ep_t[1]]*(ğ’Ÿ[:return][1, ep_t[1]] .> ğ’®.ğ’«[:f_target_train][id]) 
		
		# Update the logprobability of the (s,a) tuple for the new failure mode
		if haskey(ğ’Ÿ, :logprob)
			ğ’Ÿ[:logprob][:, ep] .= logpdf(Ï€s[id], ğ’Ÿ[:s][:,ep], ğ’Ÿ[:a][:,ep])
		end
	end
	
	if sum(new_weights) > 0
		# Set a minimum fraction for each distribution
		new_weights =  (new_weights ./ sum(new_weights)) .+ 0.1
		
		# Compute the target weight values
		new_weights = new_weights ./ sum(new_weights)
		
		# Perform a moving average update
		mis.weights += 0.05*(new_weights .- mis.weights)
		
		# Normalize to make sure the total samples is always equal to the 
		mis.weights = mis.weights ./ sum(mis.weights)
	end
	
	
	# Record the fraction of samples from each
	for i=1:length(mis.weights)
		info["index$(i)_frac"] = mis.weights[i]
	end
end

function cem_training(ğ’®::EvaluationSolver, ğ’Ÿ)
	info = Dict()
	Ï€s = all_policies(ğ’®.agent.Ï€)
	for (id, Ï€) in enumerate(Ï€s)
		@assert Ï€ isa DistributionPolicy
		ğ’Ÿid = haskey(ğ’Ÿ, :id) ? ExperienceBuffer(minibatch(ğ’Ÿ, findall(ğ’Ÿ[:id][:] .== id))) : ğ’Ÿ
		weights = ((ğ’Ÿid[:return] .> ğ’®.ğ’«[:f_target_train][id]) .* ğ’Ÿid[:traj_importance_weight])[:]
		(length(ğ’Ÿid) == 0 || sum(weights) == 0) && continue
		a = Ï€.distribution isa Normal ? ğ’Ÿid[:a][1, :] : ğ’Ÿid[:a]
		
		if Ï€.distribution isa ObjectCategorical
			Ï€.distribution = Distributions.fit(typeof(Ï€.distribution), Float64.(a), Float64.(weights), objs=Ï€.distribution.objs)
		else
			Ï€.distribution = Distributions.fit(typeof(Ï€.distribution), Float64.(a), Float64.(weights))
		end
	end
	info
end

function policy_gradient_training(ğ’®::EvaluationSolver, ğ’Ÿ)
    info = Dict()
	
	Ï€t = trainable_indices(ğ’®.agent.Ï€)
	Ï€s = all_policies(ğ’®.agent.Ï€)
	
	for id in Ï€t
		Ï€ = Ï€s[id]
		ğ’Ÿid = haskey(ğ’Ÿ, :id) ? ExperienceBuffer(minibatch_copy(ğ’Ÿ, findall(ğ’Ÿ[:id][:] .== id))) : deepcopy(ğ’Ÿ)
		ğ’®.ğ’«[:f_target_train_current][1] = ğ’®.ğ’«[:f_target_train][id]
		
		length(ğ’Ÿid) == 0 && continue
	
		# Train Actor
		batch_train!(actor(Ï€), ğ’®.a_opt, ğ’®.ğ’«, ğ’Ÿid, info=info, Ï€_loss=Ï€)
		
		# Optionally update critic
	    if !isnothing(ğ’®.c_opt)
	        batch_train!(critic(Ï€), ğ’®.c_opt, ğ’®.ğ’«, ğ’Ÿid, info=info, Ï€_loss=Ï€)
		end
	end
    
    info
end

function value_training(ğ’®::EvaluationSolver, ğ’Ÿ)
	# Batch buffer
	ğ’Ÿbatch = buffer_like(ğ’Ÿ, capacity=ğ’®.c_opt.batch_size, device=device(ğ’®.agent.Ï€))
    
    infos = []
	
	Ï€t = trainable_indices(ğ’®.agent.Ï€)
	Ï€s = all_policies(ğ’®.agent.Ï€)
	Ï€â»s = all_policies(ğ’®.agent.Ï€â»)
	
	# Loop through policies and train 1 at a time
	for id in Ï€t
		# extract the policy and target
		ğ’®.ğ’«[:f_target_train_current][1] = ğ’®.ğ’«[:f_target_train][id]
		Ï€ = Ï€s[id]
		Ï€â» = Ï€â»s[id]
		
		ğ’Ÿid = haskey(ğ’Ÿ, :id) ? ExperienceBuffer(minibatch(ğ’Ÿ, findall(ğ’Ÿ[:id][:] .== id))) : ğ’Ÿ
	
	    # Loop over the desired number of training steps
	    for epoch in 1:ğ’®.c_opt.epochs
			
			# length(ğ’Ÿid) == 0 && continue
			# rand!(ğ’Ÿbatch, ğ’Ÿid, i=ğ’®.i)
			
	        # Sample a random minibatch of ğ‘ transitions (sáµ¢, aáµ¢, ráµ¢, sáµ¢â‚Šâ‚) from ğ’Ÿ
			rand!(ğ’Ÿbatch, ğ’Ÿ, i=ğ’®.i)
	        
			
	        # Dictionary to store info from the various optimization processes
	        info = Dict()
			
	        # Compute target
	        y = ğ’®.target_fn(Ï€â», ğ’®.ğ’«, ğ’Ÿbatch, i=ğ’®.i)
			
	        # # Update priorities (for prioritized replay)
			err = cpu(ğ’®.priority_fn(Ï€, ğ’®.ğ’«, ğ’Ÿbatch, y))
	        isprioritized(ğ’Ÿ) && update_priorities!(ğ’Ÿ, ğ’Ÿbatch.indices, err)
			
	        # Train the critic
	        if ((epoch-1) % ğ’®.c_opt.update_every) == 0
	            Crux.train!(critic(Ï€), (;kwargs...) -> ğ’®.c_opt.loss(Ï€, ğ’®.ğ’«, ğ’Ÿbatch, y; kwargs...), ğ’®.c_opt, info=info)
	        end
			
			length(ğ’Ÿid) == 0 && continue
			# Sample a random minibatch of ğ‘ transitions (sáµ¢, aáµ¢, ráµ¢, sáµ¢â‚Šâ‚) from ğ’Ÿ
	        uniform_sample!(ğ’Ÿbatch, ğ’Ÿid)
	        
	        # Train the actor 
	        if !isnothing(ğ’®.a_opt) && ((epoch-1) % ğ’®.a_opt.update_every) == 0
	            Crux.train!(actor(Ï€), (;kwargs...) -> ğ’®.a_opt.loss(Ï€, ğ’®.ğ’«, ğ’Ÿbatch; kwargs...), ğ’®.a_opt, info=info)
				
	            # Update the target network
	            ğ’®.target_update(Ï€â», Ï€)
	        end
	        
	        # Store the training information
	        push!(infos, info)
	        
	    end
	    # If not using a separate actor, update target networks after critic training
	    isnothing(ğ’®.a_opt) && ğ’®.target_update(Ï€â», Ï€, i=ğ’®.i + 1:ğ’®.i + ğ’®.Î”N)
	end
    
    aggregate_info(infos)
end

function POMDPs.solve(ğ’®::EvaluationSolver, mdp)
	try mkdir("frames") catch end
	@assert haskey(ğ’®.ğ’«, :f_target)
	
	# Pre-train the policy if a function is provided
	if !isnothing(ğ’®.agent_pretrain) && ğ’®.i == 0
		ğ’®.agent_pretrain(ğ’®.agent.Ï€)
		if !isnothing(ğ’®.agent.Ï€â»)
			ğ’®.agent.Ï€â»=deepcopy(ğ’®.agent.Ï€)
		end 
	end
	
	# Construct the training buffer
	# TODO: consider different buffers for VB replay?
	ğ’®.ğ’Ÿ = buffer_like(ğ’®.buffer, capacity=ğ’®.training_buffer_size, device=device(ğ’®.agent.Ï€))
	
    # Construct the training buffer, constants, and sampler
    s = Sampler(mdp, ğ’®.agent, S=ğ’®.S, required_columns=ğ’®.required_columns, max_steps=ğ’®.max_steps, traj_weight_fn=ğ’®.weight_fn)
    !isnothing(ğ’®.log) && isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s)

    # Log the pre-train performance
    log(ğ’®.log, ğ’®.i, ğ’®=ğ’®)

    # Loop over the desired number of environment interactions
    for ğ’®.i = range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Info to collect during training
        info = Dict()
        
        # Sample transitions into the batch buffer
		@assert length(ğ’®.buffer) < capacity(ğ’®.buffer) # Make sure we never overwrite
		start_index=length(ğ’®.buffer) + 1
		ğ’®.training_type in [:policy_gradient, :cem] && clear!(ğ’®.ğ’Ÿ)
		episodes!(s, ğ’®.ğ’Ÿ, store=ğ’®.buffer, Neps=ğ’®.Î”N, explore=true, i=ğ’®.i, cb=(D) -> ğ’®.post_sample_callback(D, info=info, ğ’®=ğ’®))
		end_index=length(ğ’®.buffer)
		
		# Record the average weight of the samples
		ep_ends = ğ’®.buffer[:episode_end][1,start_index:end_index]
		info[:mean_weight] = sum(ğ’®.buffer[:traj_importance_weight][1,start_index:end_index][ep_ends]) / sum(ep_ends)
		@assert !isnan(info[:mean_weight])

		# If we are training then update required values and train
		training_info = Dict()
		if ğ’®.training_type != :none
		
			# Assign each datapoint in the training batch to one of the policies
			assign_mode_ids(ğ’®, ğ’®.ğ’Ÿ; info)
			
			# gradually increase the target
	        gradual_target_increase(ğ’®, ğ’®.ğ’Ÿ; info)
			
			# Plot training frames
			# try
			# 	if mod(ğ’®.i, 100) == 0
			# 		function plot_traj(Ï€; label, p=plot())
			# 			D = episodes!(Sampler(mdp, Ï€), Neps=10)
			# 			scatter!(p, D[:s][1, :], D[:s][2, :], label=label)
			# 		end
			# 		ps = []
			# 
			# 		for (i,Ï€) in enumerate(all_policies(ğ’®.agent.Ï€))
			# 			if Ï€ isa ActorCritic
			# 				if ğ’®.training_type == :policy_gradient
			# 					p = heatmap(0:0.1:2, -1.2:0.1:1.2, (t,Î¸) -> value(Ï€, [t, Î¸, 0f0])[1], clims=(0,1))
			# 				else
			# 					p = heatmap(0:0.1:2, -1.2:0.1:1.2, (t,Î¸) -> value(Ï€, [t, Î¸, 0f0, 0f0])[1], clims=(0,1))
			# 				end
			# 			else 
			# 				p = plot(ylims=(-1.2,1.2))
			# 			end
			# 			plot_traj(Ï€, label="q$i", p=p)
			# 			push!(ps, p)
			# 		end
			# 
			# 		p=plot(ps..., layout=(length(ps), 1), size=(600, 200*length(ps)))
			# 		savefig(p, "frames/frame$(ğ’®.i).png")
			# 	end
			# catch end
			
	        # Train the networks
	        if ğ’®.training_type == :policy_gradient
	            training_info = policy_gradient_training(ğ’®, ğ’®.ğ’Ÿ)
	        elseif ğ’®.training_type == :value
	            training_info = value_training(ğ’®, ğ’®.ğ’Ÿ)
			elseif ğ’®.training_type == :cem
				training_info = cem_training(ğ’®, ğ’®.ğ’Ÿ)
	        else
	            @error "uncregonized training type: $training_type"
	        end
		end
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, info, training_info,  ğ’®=ğ’®)
    end
    ğ’®.i += ğ’®.Î”N
	
	# Extract the samples
	eps = episodes(ğ’®.buffer)
	fs = [ğ’®.buffer[:return][1, ep[1]] > ğ’®.ğ’«[:f_target][1] for ep in eps]
	ws = [ğ’®.buffer[:traj_importance_weight][1, ep[1]] for ep in eps]
	
    fs, ws
end

