using Parameters, Random
import Zygote: ignore_derivatives

@with_kw mutable struct EvaluationSolver <: Solver
    agent::PolicyParams # Policy
    S::AbstractSpace # State space
    N::Int = 1000 # Number of episode samples
    Î”N::Int = 200 # Number of episode samples between updates
    max_steps::Int = 100 # Maximum number of steps per episode
    log::Union{Nothing, LoggerParams} = nothing # The logging parameters
    i::Int = 0 # The current number of episode interactions
    a_opt::Union{Nothing, TrainingParams} = nothing# Training parameters for the actor
    c_opt::Union{Nothing, TrainingParams} = nothing # Training parameters for the critic
    ğ’«::NamedTuple = (;) # Parameters of the algorithm
    post_sample_callback = (ğ’Ÿ; kwargs...) -> nothing # Callback that that happens after sampling experience
    pre_train_callback = (ğ’®; kwargs...) -> nothing # callback that gets called once prior to training
    required_columns = Symbol[:traj_importance_weight, :return]
	
	# Stuff specific to estimation
	training_type = :none # Type of training loop, either :policy_gradient, :value, :none
	recent_batch_only = true # Whether or not to train on all prior data or just the recent batch
	weight_fn = (agent, data, ep) -> trajectory_pdf(agent.pa, data, ep) / trajectory_pdf(agent.Ï€, data, ep)
	agent_pretrain=nothing # Function to pre-train the agent before any rollouts

	# Create buffer to store all of the samples
	buffer_size = N*max_steps
	buffer = ExperienceBuffer(S, agent.space, buffer_size, required_columns)

    # Stuff for off policy update
    target_update = (Ï€â», Ï€; kwargs...) -> polyak_average!(Ï€â», Ï€, 0.005f0) # Function for updating the target network
    target_fn=nothing # Target for critic regression with input signature (Ï€â», ğ’Ÿ, Î³; i)
    priority_fn = Crux.td_error  # function for prioritized replay
end

MCSolver(args...; kwargs...) = EvaluationSolver(args...; kwargs...)

function gradual_target_increase(ğ’®, ğ’Ÿ; info)
	fs = ğ’Ÿ[:return][ğ’Ÿ[:episode_end] .== true][:]
	start = max(1, length(fs) - ğ’®.ğ’«[:N_elite_candidate])
	fs = fs[start:end]

	elite_target_min = sort(fs, rev=true)[max(1, floor(Int, length(fs) * ğ’®.ğ’«[:elite_frac]))]
	target = min(elite_target_min, ğ’®.ğ’«[:f_target][1])
	info[:f_target_train] = target
	ğ’®.ğ’«[:f_target_train][1] = target
end

function assign_mode_ids(ğ’®, ğ’Ÿ; info=Dict())
		Ï€s = trainable_policies(ğ’®.agent.Ï€)
		length(Ï€s) == 1 && return # If there is only one trainable policy, only it will be trained
		eps = episodes(ğ’Ÿ)
		for ep_t in eps
			if ğ’Ÿ[:id][1, ep_t[1]] == 0
				ep = ep_t[1]:ep_t[2]
				# update the failure mode id of each sample (this might change over time as well)
				id = argmax([trajectory_pdf(d, ğ’Ÿ, ep) for d in Ï€s])
				ğ’Ÿ[:id][1, ep] .= id
		
				# Update the logprobabiliyt of the (s,a) tuple for the new failure mode
				if haskey(ğ’Ÿ, :logprob)
					ğ’Ÿ[:logprob][:, ep] .= logpdf(ğ’®.agent.Ï€.distributions[id], ğ’Ÿ[:s][:,ep], ğ’Ÿ[:a][:,ep])
				end
			end
		end
		
		# Record the fraction of samples from each
		for i=1:length(Ï€s)
			info["index$(i)_frac"] = sum(ğ’Ÿ[:id] .== i) / length(ğ’Ÿ[:id])
		end
end

function policy_gradient_training(ğ’®::EvaluationSolver, buffer)
    info = Dict()
    
    # Train Actor
    batch_train!(actor(ğ’®.agent.Ï€), ğ’®.a_opt, ğ’®.ğ’«, buffer, info=info, Ï€_loss=ğ’®.agent.Ï€)
    
    # Optionally update critic
    if !isnothing(ğ’®.c_opt)
        batch_train!(critic(ğ’®.agent.Ï€), ğ’®.c_opt, ğ’®.ğ’«, buffer, info=info, Ï€_loss=ğ’®.agent.Ï€)
    end
    
    info
end

function value_training(ğ’®::EvaluationSolver, buffer)
    ğ’Ÿ = buffer_like(buffer, capacity=ğ’®.c_opt.batch_size, device=device(ğ’®.agent.Ï€))
    
    infos = []
	
    # Loop over the desired number of training steps
    for epoch in 1:ğ’®.c_opt.epochs
        # Sample a random minibatch of ğ‘ transitions (sáµ¢, aáµ¢, ráµ¢, sáµ¢â‚Šâ‚) from ğ’Ÿ
        rand!(ğ’Ÿ, buffer, i=ğ’®.i)
        
        # Dictionary to store info from the various optimization processes
        info = Dict()
        
        # Compute target
        y = ğ’®.target_fn(ğ’®.agent.Ï€â», ğ’®.ğ’«, ğ’Ÿ, i=ğ’®.i)
        
        # # Update priorities (for prioritized replay)
        # isprioritized(ğ’®.buffer) && update_priorities!(ğ’®.buffer, ğ’Ÿ.indices, cpu(ğ’®.priority_fn(ğ’®.agent.Ï€, ğ’®.ğ’«, ğ’Ÿ, y)))
        
        # Train the critic
        if ((epoch-1) % ğ’®.c_opt.update_every) == 0
            Crux.train!(critic(ğ’®.agent.Ï€), (;kwargs...) -> ğ’®.c_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, ğ’Ÿ, y; kwargs...), ğ’®.c_opt, info=info)
        end
        
        # Train the actor 
        if !isnothing(ğ’®.a_opt) && ((epoch-1) % ğ’®.a_opt.update_every) == 0
            Crux.train!(actor(ğ’®.agent.Ï€), (;kwargs...) -> ğ’®.a_opt.loss(ğ’®.agent.Ï€, ğ’®.ğ’«, ğ’Ÿ; kwargs...), ğ’®.a_opt, info=info)
        
            # Update the target network
            ğ’®.target_update(ğ’®.agent.Ï€â», ğ’®.agent.Ï€)
        end
        
        # Store the training information
        push!(infos, info)
        
    end
    # If not using a separate actor, update target networks after critic training
    isnothing(ğ’®.a_opt) && ğ’®.target_update(ğ’®.agent.Ï€â», ğ’®.agent.Ï€, i=ğ’®.i + 1:ğ’®.i + ğ’®.Î”N)
    
    aggregate_info(infos)
end

function POMDPs.solve(ğ’®::EvaluationSolver, mdp)
	@assert haskey(ğ’®.ğ’«, :f_target)
	
	# Pre-train the policy if a function is provided
	if !isnothing(ğ’®.agent_pretrain) && ğ’®.i == 0
		ğ’®.agent_pretrain(ğ’®.agent.Ï€)
		if !isnothing(ğ’®.agent.Ï€â»)
			ğ’®.agent.Ï€â»=deepcopy(ğ’®.agent.Ï€)
		end 
	end
	
    # Construct the training buffer, constants, and sampler
    s = Sampler(mdp, ğ’®.agent, S=ğ’®.S, required_columns=ğ’®.required_columns, max_steps=ğ’®.max_steps, traj_weight_fn=ğ’®.weight_fn)
    !isnothing(ğ’®.log) && isnothing(ğ’®.log.sampler) && (ğ’®.log.sampler = s)

    # Log the pre-train performance
    log(ğ’®.log, ğ’®.i, ğ’®=ğ’®)

    # Loop over the desired number of environment interactions
    for ğ’®.i = range(ğ’®.i, stop=ğ’®.i + ğ’®.N - ğ’®.Î”N, step=ğ’®.Î”N)
        # Info to collect during training
        info = Dict()
        
        # Sample transitions into the batch buffer
		@assert length(ğ’®.buffer) < capacity(ğ’®.buffer) # Make sure we never overwrite
		start_index=length(ğ’®.buffer) + 1
		episodes!(s, ğ’®.buffer, Neps=ğ’®.Î”N, explore=true, i=ğ’®.i, cb=(D) -> ğ’®.post_sample_callback(D, info=info, ğ’®=ğ’®))
		end_index=length(ğ’®.buffer)
		
		# Record the average weight of the samples
		ep_ends = ğ’®.buffer[:episode_end][1,start_index:end_index]
		info[:mean_weight] = sum(ğ’®.buffer[:traj_importance_weight][1,start_index:end_index][ep_ends]) / sum(ep_ends)
		@assert !isnan(info[:mean_weight])

		# If we are training then update required values and train
		training_info = Dict()
		if ğ’®.training_type != :none
		
			ğ’Ÿ = ğ’®.recent_batch_only ? ExperienceBuffer(minibatch(ğ’®.buffer, start_index:end_index)) : ğ’®.buffer
			
			# Pre-train callback, used to make changes to the buffer and update f
	        ğ’®.pre_train_callback(ğ’®, ğ’Ÿ; info)
			
			ğ’®.agent.Ï€ isa MISPolicy && assign_mode_ids(ğ’®, ğ’Ÿ; info)
			
			
	        # Train the networks
	        if ğ’®.training_type == :policy_gradient
	            training_info = policy_gradient_training(ğ’®, ğ’Ÿ)
	        elseif ğ’®.training_type == :value
	            training_info = value_training(ğ’®, ğ’Ÿ)
	        else
	            @error "uncregonized training type: $training_type"
	        end
		end
        
        # Log the results
        log(ğ’®.log, ğ’®.i + 1:ğ’®.i + ğ’®.Î”N, info, training_info, ğ’®=ğ’®)
    end
    ğ’®.i += ğ’®.Î”N
	
	# Extract the samples
	eps = episodes(ğ’®.buffer)
	fs = [ğ’®.buffer[:return][1, ep[1]] > ğ’®.ğ’«[:f_target][1] for ep in eps]
	ws = [ğ’®.buffer[:traj_importance_weight][1, ep[1]] for ep in eps]
	
    fs, ws
end

